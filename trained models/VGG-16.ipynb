{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BP9xNnTfq5y"
      },
      "source": [
        "# **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH02RhpJ65ET"
      },
      "outputs": [],
      "source": [
        "# Extract ZIP Files\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/neudefectdataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/neudataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiJi3EdPf0ne"
      },
      "source": [
        "# **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hOmBBkTyqow"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Input, Lambda, Dense, Flatten, Dropout\n",
        "from keras.models import Model\n",
        "from keras import optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-Ou8r81f5x9"
      },
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czB3UH7oyyof",
        "outputId": "0546a2d7-bd65-45d5-8a7c-971d89c755e5"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "dir = '/content/neudataset/neudefectdataset'\n",
        "data = pd.read_csv('/content/drive/MyDrive/CSVNEU224.csv')\n",
        "\n",
        "image_filenames = data['image'].values\n",
        "labels = data['label'].values\n",
        "\n",
        "images = []\n",
        "for filename, label in zip(image_filenames, labels):\n",
        "    image_path = os.path.join(dir, label, filename)\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is not None:\n",
        "        images.append(image)\n",
        "    else:\n",
        "        print(f\"Failed to load image: {image_path}\")\n",
        "\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPoZKXJBcGm4",
        "outputId": "3bbc29eb-9917-41bd-9e63-1cf6ecc2883b"
      },
      "outputs": [],
      "source": [
        "# Transform string labels into numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "print(\"Label Mapping:\", label_mapping)\n",
        "\n",
        "X_subset, X_test, y_subset, y_test = train_test_split(images, labels, test_size=0.15, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3L_6ysTgaIv"
      },
      "source": [
        "# **Data Sample**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "r7BGlPgben-4",
        "outputId": "2392cc04-d2b2-4b29-8536-26db3e82ae0b"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X_test[100][:,:,0])\n",
        "print(y_test[100])\n",
        "\n",
        "print(X_subset.shape)\n",
        "print(X_test.shape)\n",
        "print(y_subset.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWxOGYdTRNr-"
      },
      "source": [
        "# **Model development**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4wFeEaSHK66"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "num_classes = 6\n",
        "epochs = 15\n",
        "input_shape = (224,224,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH-r60QO3Z52",
        "outputId": "1ca733b1-3ac6-45b5-e6dc-41cb61d7a7cf"
      },
      "outputs": [],
      "source": [
        "# Learning rates for the grid search\n",
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "\n",
        "# List to store the trained models\n",
        "trained_models = []\n",
        "\n",
        "# Stratified K Fold Cross Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Lists to store all performance metrics\n",
        "all_accuracies = []\n",
        "all_tp_rates = []\n",
        "all_ppv_values = []\n",
        "\n",
        "all_fold_accuracies = []\n",
        "all_fold_tp_rates = []\n",
        "all_fold_ppv_values = []\n",
        "\n",
        "# Perform grid search\n",
        "for lr in learning_rates:\n",
        "    # Lists to store performance metrics for each fold\n",
        "    accuracies = []\n",
        "    tp_rates = []\n",
        "    ppv_values = []\n",
        "\n",
        "    for fold, (train, val) in enumerate(skf.split(X_subset, y_subset), start=1):\n",
        "      X_tr = X_subset[train]\n",
        "      y_tr = y_subset[train]\n",
        "      X_val = X_subset[val]\n",
        "      y_val = y_subset[val]\n",
        "\n",
        "      encoder = OneHotEncoder()\n",
        "      y_subset_encoded = encoder.fit_transform(y_subset.reshape(-1, 1)).toarray()\n",
        "\n",
        "      y_tr = y_subset_encoded[train]  # One hot vectors\n",
        "      y_val = y_subset_encoded[val]   # One hot vectors\n",
        "\n",
        "      # Print the fold number and learning rate\n",
        "      print(f\"Fold {fold}, Learning Rate: {lr}\")\n",
        "\n",
        "      # Model architecture + Transfer Learning\n",
        "      vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "      for layer in vgg16.layers:\n",
        "          layer.trainable = False\n",
        "\n",
        "      x = Flatten()(vgg16.output)\n",
        "      x = Dense(128, activation='relu')(x)\n",
        "      x = Dropout(0.2)(x)\n",
        "      x = Dense(64, activation='relu')(x)\n",
        "      output = Dense(num_classes, activation='softmax')(x)\n",
        "      model = Model(inputs=vgg16.input, outputs=output)\n",
        "\n",
        "      model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "                metrics=['accuracy'])\n",
        "      model.fit(X_tr, y_tr, validation_data=(X_val, y_val),\n",
        "                batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "      # Evaluate the model on validation data\n",
        "      y_pred = model.predict(X_val)\n",
        "\n",
        "      y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "      y_val_categorical = np.argmax(y_val, axis=1)\n",
        "\n",
        "      # Calculate accuracy, precision, and recall\n",
        "      accuracy = accuracy_score(y_val_categorical, y_pred_classes)\n",
        "      precision = precision_score(y_val_categorical, y_pred_classes, average='macro')\n",
        "      recall = recall_score(y_val_categorical, y_pred_classes, average='macro')\n",
        "\n",
        "      # Store accuracy, recall, and precision values\n",
        "      accuracies.append(accuracy)\n",
        "      tp_rates.append(recall)\n",
        "      ppv_values.append(precision)\n",
        "\n",
        "      # All folds\n",
        "      all_fold_accuracies.append(accuracy)\n",
        "      all_fold_tp_rates.append(recall)\n",
        "      all_fold_ppv_values.append(precision)\n",
        "\n",
        "      # Store trained models\n",
        "      trained_models.append(model)\n",
        "\n",
        "    # Calculate the average and standard deviation of evaluation metrics\n",
        "    average_accuracy = np.mean(accuracies)\n",
        "    std_accuracy = np.std(accuracies)\n",
        "\n",
        "    average_tp_rate = np.mean(tp_rates, axis=0)\n",
        "    std_tp_rate = np.std(tp_rates, axis=0)\n",
        "\n",
        "    average_ppv = np.mean(ppv_values, axis=0)\n",
        "    std_ppv = np.std(ppv_values, axis=0)\n",
        "\n",
        "    # Store all values\n",
        "    all_accuracies.append(average_accuracy)\n",
        "    all_tp_rates.append(average_tp_rate)\n",
        "    all_ppv_values.append(average_ppv)\n",
        "\n",
        "    # Print or store the average and standard deviation of evaluation metrics\n",
        "    print(\"Average Accuracy:\", average_accuracy)\n",
        "    print(\"Standard Deviation of Accuracy:\", std_accuracy)\n",
        "\n",
        "    print(\"Average Recall:\", average_tp_rate)\n",
        "    print(\"Standard Deviation of Recall:\", std_tp_rate)\n",
        "\n",
        "    print(\"Average Positive Precision:\", average_ppv)\n",
        "    print(\"Standard Deviation of Precision:\", std_ppv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rZ_9wBpzHWj",
        "outputId": "33e245a4-700b-4690-e336-a83aba6e89db"
      },
      "outputs": [],
      "source": [
        "# Print all the stored performance metric values for all folds and learning rate values\n",
        "# Evaluated on VALIDATION DATA\n",
        "\n",
        "print(\"All Accuracies:\", all_fold_accuracies)\n",
        "print(\"All Avg TP Rates:\", all_fold_tp_rates)\n",
        "print(\"All Avg PPV Values:\", all_fold_ppv_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ6evjAUCq-0"
      },
      "outputs": [],
      "source": [
        "# Get models with the highest accuracy performance, grouped by learning rate\n",
        "\n",
        "tfmodels = trained_models[10:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azK-aUK1TEQ1",
        "outputId": "464f3566-423b-4588-b0df-3a517a9496ec"
      },
      "outputs": [],
      "source": [
        "# Print & store all performance metrics for all folds and learning rate values\n",
        "# Evaluated on TEST DATA\n",
        "\n",
        "\n",
        "# Lists to store evaluation results on test data for all models\n",
        "all_test_accuracies = []\n",
        "all_test_tp_rates = []\n",
        "all_test_ppv_values = []\n",
        "\n",
        "for model in tfmodels:\n",
        "    # Initialize lists for this model\n",
        "    test_accuracies = []\n",
        "    test_tp_rates = []\n",
        "    test_ppv_values = []\n",
        "\n",
        "    # Evaluate the model on test data\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "    test_precision = precision_score(y_test, y_pred_classes, average='macro')\n",
        "    test_recall = recall_score(y_test, y_pred_classes, average='macro')\n",
        "\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    test_tp_rates.append(test_recall)\n",
        "    test_ppv_values.append(test_precision)\n",
        "\n",
        "\n",
        "    # Append results for this model to the corresponding all_test lists\n",
        "    all_test_accuracies.append(test_accuracies)\n",
        "    all_test_tp_rates.append(test_tp_rates)\n",
        "    all_test_ppv_values.append(test_ppv_values)\n",
        "\n",
        "print(\" \")\n",
        "print(\"All Test Accuracies:\", all_test_accuracies)\n",
        "print(\"All Test Recall:\", all_test_tp_rates)\n",
        "print(\"All Test Precision:\", all_test_ppv_values)\n",
        "print(\" \")\n",
        "print(\"Mean Test Accuracies:\", np.mean(all_test_accuracies))\n",
        "print(\"Mean Test Recall:\", np.mean(all_test_tp_rates))\n",
        "print(\"Mean Test Precision:\", np.mean(all_test_ppv_values))\n",
        "print(\" \")\n",
        "print(\"Std Dev Accuracies:\", np.std(all_test_accuracies))\n",
        "print(\"Std Dev Recall:\", np.std(all_test_tp_rates))\n",
        "print(\"Std Dev Precision:\", np.std(all_test_ppv_values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck3dwvXZHi-t"
      },
      "source": [
        "# **Saving the weights of models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNQrJ3jnDD_Q",
        "outputId": "8a550224-a624-4fd3-ce21-3b24661ec3ec"
      },
      "outputs": [],
      "source": [
        "# Save models to disk\n",
        "for i, model in enumerate(tfmodels, start=1):\n",
        "    filename = f\"model{i}.h5\"\n",
        "    model.save(filename)\n",
        "    print(f\"Saved {filename} to disk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_M6zqxvZkx3",
        "outputId": "791dd23c-1260-4325-dd84-efbe0736dc31"
      },
      "outputs": [],
      "source": [
        "orig_model_size = []\n",
        "\n",
        "for i in range(0,5):\n",
        "  # Model size of the original model\n",
        "  orig_model_size_mb = os.path.getsize(f'model{i+1}.h5') / float(2**20)\n",
        "  orig_model_size.append(orig_model_size_mb)\n",
        "\n",
        "mean_orig_model_size = np.mean(orig_model_size)\n",
        "std_orig_model_size = np.std(orig_model_size)\n",
        "\n",
        "print(\"Average original model size: \", mean_orig_model_size)\n",
        "print(\"Std Dev original model size: \", std_orig_model_size)\n",
        "\n",
        "print(\"Original model sizes: \", orig_model_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szl5L3Vml0h9"
      },
      "source": [
        "# **Running the TFLite converted & Quantized Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOEzI--W10UW",
        "outputId": "eb19ca82-518d-4b62-89d5-58dd060b982d"
      },
      "outputs": [],
      "source": [
        "# TFLite Model Float32 Inference & Performance\n",
        "\n",
        "tflite_model_sizes = []\n",
        "tflite_inference_times = []\n",
        "tflite_accuracies = []\n",
        "tflite_recalls = []\n",
        "tflite_precisions = []\n",
        "\n",
        "for i, model in enumerate(tfmodels):\n",
        "\n",
        "    # Convert the model to TFLite\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    # Save the TFLite model to a file\n",
        "    tflite_model_path = f'model_{i+1}_lite.tflite'\n",
        "    with open(tflite_model_path, 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "\n",
        "    # Load the quantized TFLite model\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    num_classes = 6\n",
        "\n",
        "    # Calculate metrics\n",
        "    correct_count = 0\n",
        "    total_count = 0\n",
        "    inference_times = []\n",
        "\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    for x, true_label in zip(X_test, y_test):\n",
        "        x = x.astype(np.float32)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "\n",
        "        interpreter.set_tensor(input_details[0]['index'], x)\n",
        "\n",
        "        start_time = time.time()\n",
        "        interpreter.invoke()\n",
        "        end_time = time.time()\n",
        "\n",
        "        inf_time = (end_time - start_time)\n",
        "        inference_times.append(inf_time)\n",
        "\n",
        "        output = interpreter.get_tensor(output_details[0]['index'])\n",
        "        predicted_label = np.argmax(output)\n",
        "        y_pred.append(predicted_label)\n",
        "        y_true.append(true_label)\n",
        "\n",
        "        if true_label == predicted_label:\n",
        "            correct_count += 1\n",
        "        total_count += 1\n",
        "\n",
        "    # Model size of the lite model\n",
        "    model_size_mb = os.path.getsize(f'model_{i+1}_lite.tflite') / float(2**20)\n",
        "\n",
        "    # Accuracy of lite model\n",
        "    accuracy = (correct_count / total_count) * 100\n",
        "\n",
        "    # Calculate precision\n",
        "    precision = precision_score(y_true, y_pred, average='macro') * 100\n",
        "\n",
        "    # Calculate recall\n",
        "    recall = recall_score(y_true, y_pred, average='macro') * 100\n",
        "\n",
        "    # Inference time of model\n",
        "    lite_inference_time = np.mean(inference_times)\n",
        "\n",
        "    # Append metrics to lists\n",
        "    tflite_model_sizes.append(model_size_mb)\n",
        "    tflite_accuracies.append(accuracy)\n",
        "    tflite_precisions.append(precision)\n",
        "    tflite_recalls.append(recall)\n",
        "    tflite_inference_times.append(lite_inference_time)\n",
        "\n",
        "mean_model_size = np.mean(tflite_model_sizes)\n",
        "std_model_size = np.std(tflite_model_sizes)\n",
        "\n",
        "mean_accuracy = np.mean(tflite_accuracies)\n",
        "std_accuracy = np.std(tflite_accuracies)\n",
        "\n",
        "mean_precision = np.mean(tflite_precisions)\n",
        "std_precision = np.std(tflite_precisions)\n",
        "\n",
        "mean_recall = np.mean(tflite_recalls)\n",
        "std_recall = np.std(tflite_recalls)\n",
        "\n",
        "mean_inference_time = np.mean(tflite_inference_times)\n",
        "std_inference_time = np.std(tflite_inference_times)\n",
        "\n",
        "# Print the mean and standard deviation\n",
        "print(f\"Average Float32 Model Size: {mean_model_size} Mb, Std Dev Float32 Model Size: {std_model_size} Mb\")\n",
        "print(f\"Average Float32 Accuracy: {mean_accuracy}, Std Dev FFloat32 Accuracy: {std_accuracy}\")\n",
        "print(f\"Average Float32 Precision: {mean_precision}, Std Dev Float32 Precision: {std_precision}\")\n",
        "print(f\"Average Float32 Recall: {mean_recall}, Std Dev Float32 Recall: {std_recall}\")\n",
        "print(f\"Average Float32 Inference Time: {mean_inference_time} seconds, Std Dev Float32 Inference Time: {std_inference_time} seconds\")\n",
        "print(\" \")\n",
        "\n",
        "print(\"Float32 Model sizes: \", tflite_model_sizes)\n",
        "print(\"Float32 Accuracies: \", tflite_accuracies)\n",
        "print(\"Float32 Precisions: \", tflite_precisions)\n",
        "print(\"Float32 Recalls: \", tflite_recalls)\n",
        "print(\"Float32 Inference Times: \", tflite_inference_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg1AfGTlZhIz",
        "outputId": "5959743c-6acf-4881-b07b-066c928daf0e"
      },
      "outputs": [],
      "source": [
        "# Quantized TFLite Model Float16 Inference & Performance\n",
        "\n",
        "float16_model_sizes = []\n",
        "float16_accuracies = []\n",
        "float16_precisions = []\n",
        "float16_recalls = []\n",
        "float16_inference_times = []\n",
        "\n",
        "for i, model in enumerate(tfmodels):\n",
        "\n",
        "    # Convert the model to TFLite\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_types = [tf.float16]  # PTQ float16\n",
        "    tflite_quant_model = converter.convert()\n",
        "\n",
        "    # Save the TFLite model to a file\n",
        "    tflite_model_path = f'model_{i+1}_float16quantized.tflite'\n",
        "    with open(tflite_model_path, 'wb') as f:\n",
        "        f.write(tflite_quant_model)\n",
        "\n",
        "    # Load the quantized TFLite model\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    num_classes = 6\n",
        "\n",
        "    # Calculate metrics\n",
        "    correct_count = 0\n",
        "    total_count = 0\n",
        "    inference_times = []\n",
        "\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    for x, true_label in zip(X_test, y_test):\n",
        "        x = x.astype(np.float32)  # Convert input data to FLOAT32\n",
        "        x = np.expand_dims(x, axis=0)  # Add batch dimension\n",
        "\n",
        "        interpreter.set_tensor(input_details[0]['index'], x)\n",
        "\n",
        "        start_time = time.time()\n",
        "        interpreter.invoke()\n",
        "        end_time = time.time()\n",
        "\n",
        "        inf_time = (end_time - start_time)\n",
        "        inference_times.append(inf_time)\n",
        "\n",
        "        output = interpreter.get_tensor(output_details[0]['index'])\n",
        "        predicted_label = np.argmax(output)\n",
        "        y_pred.append(predicted_label)\n",
        "        y_true.append(true_label)\n",
        "\n",
        "        if true_label == predicted_label:\n",
        "            correct_count += 1\n",
        "        total_count += 1\n",
        "\n",
        "    # Model size of the quantized model\n",
        "    model_size_mb = os.path.getsize(f'model_{i+1}_float16quantized.tflite') / float(2**20)\n",
        "\n",
        "    # Accuracy of quantized model\n",
        "    accuracy = (correct_count / total_count) * 100\n",
        "\n",
        "    # Calculate precision\n",
        "    precision = precision_score(y_true, y_pred, average='macro') * 100\n",
        "\n",
        "    # Calculate recall\n",
        "    recall = recall_score(y_true, y_pred, average='macro') * 100\n",
        "\n",
        "    # Inference time of model\n",
        "    quantized_inference_time = np.mean(inference_times)\n",
        "\n",
        "    # Append metrics to lists\n",
        "    float16_model_sizes.append(model_size_mb)\n",
        "    float16_accuracies.append(accuracy)\n",
        "    float16_precisions.append(precision)\n",
        "    float16_recalls.append(recall)\n",
        "    float16_inference_times.append(quantized_inference_time)\n",
        "\n",
        "mean_model_size = np.mean(float16_model_sizes)\n",
        "std_model_size = np.std(float16_model_sizes)\n",
        "\n",
        "mean_accuracy = np.mean(float16_accuracies)\n",
        "std_accuracy = np.std(float16_accuracies)\n",
        "\n",
        "mean_precision = np.mean(float16_precisions)\n",
        "std_precision = np.std(float16_precisions)\n",
        "\n",
        "mean_recall = np.mean(float16_recalls)\n",
        "std_recall = np.std(float16_recalls)\n",
        "\n",
        "mean_inference_time = np.mean(float16_inference_times)\n",
        "std_inference_time = np.std(float16_inference_times)\n",
        "\n",
        "# Print the mean and standard deviation\n",
        "print(f\"Average Float16 Model Size: {mean_model_size} Mb, Std Dev Float16 Model Size: {std_model_size} Mb\")\n",
        "print(f\"Average Float16 Accuracy: {mean_accuracy}, Std Dev Float16 Accuracy: {std_accuracy}\")\n",
        "print(f\"Average Float16 Precision: {mean_precision}, Std Dev Float16 Precision: {std_precision}\")\n",
        "print(f\"Average Float16 Recall: {mean_recall}, Std Dev Float16 Recall: {std_recall}\")\n",
        "print(f\"Average Float16 Inference Time: {mean_inference_time} seconds, Std Dev Float16 Inference Time: {std_inference_time} seconds\")\n",
        "print(\" \")\n",
        "\n",
        "print(\"Float16 Model sizes: \", float16_model_sizes)\n",
        "print(\"Float16 Accuracies: \", float16_accuracies)\n",
        "print(\"Float16 Precisions: \", float16_precisions)\n",
        "print(\"Float16 Recalls: \", float16_recalls)\n",
        "print(\"Float16 Inference Times: \", float16_inference_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2bn62EPdC0M",
        "outputId": "9d21086d-506a-4600-c032-003eb75fff3c"
      },
      "outputs": [],
      "source": [
        "# Quantized TFLite Model Full Integer 8-bit Inference & Performance\n",
        "\n",
        "num_calibration_steps = 500\n",
        "\n",
        "def representative_dataset():\n",
        "    np.random.shuffle(X_tr)\n",
        "    for i in range(num_calibration_steps):\n",
        "        sample = X_tr[i]  # X_train is in the format (height, width, channels)\n",
        "        sample = cv2.resize(sample, (224, 224))  # Resize to (224, 224)\n",
        "        sample = sample.astype(np.float32)  # Convert to FLOAT32\n",
        "        sample = np.expand_dims(sample, axis=0)  # Add batch dimension\n",
        "        yield [sample]\n",
        "\n",
        "int8_model_sizes = []\n",
        "int8_accuracies = []\n",
        "int8_precisions = []\n",
        "int8_recalls = []\n",
        "int8_inference_times = []\n",
        "\n",
        "for i, model in enumerate(tfmodels):\n",
        "\n",
        "    # Convert the model to TensorFlow Lite format with quantization\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_dataset\n",
        "    tflite_quant8_model = converter.convert()\n",
        "\n",
        "    # Save the TFLite model to a file\n",
        "    tflite_8bit_model_path = f'model_{i+1}_int8.tflite'\n",
        "    with open(tflite_8bit_model_path, 'wb') as f:\n",
        "        f.write(tflite_quant8_model)\n",
        "\n",
        "    # Load the quantized TFLite model\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_8bit_model_path)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    num_classes = 6\n",
        "\n",
        "    # Calculate metrics\n",
        "    correct_count = 0\n",
        "    total_count = 0\n",
        "    inference_times = []\n",
        "\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    for x, true_label in zip(X_test, y_test):\n",
        "        x = x.astype(np.float32)  # Convert input data to FLOAT32\n",
        "        x = np.expand_dims(x, axis=0)  # Add batch dimension\n",
        "\n",
        "        interpreter.set_tensor(input_details[0]['index'], x)\n",
        "\n",
        "        start_time = time.time()\n",
        "        interpreter.invoke()\n",
        "        end_time = time.time()\n",
        "\n",
        "        inf_time = (end_time - start_time)\n",
        "        inference_times.append(inf_time)\n",
        "\n",
        "        output = interpreter.get_tensor(output_details[0]['index'])\n",
        "        predicted_label = np.argmax(output)\n",
        "        y_pred.append(predicted_label)\n",
        "        y_true.append(true_label)\n",
        "\n",
        "        if true_label == predicted_label:\n",
        "            correct_count += 1\n",
        "        total_count += 1\n",
        "\n",
        "    # Model size of the quantized model\n",
        "    model_size_mb = os.path.getsize(f'model_{i+1}_int8.tflite') / float(2**20)\n",
        "\n",
        "    # Accuracy of quantized model\n",
        "    accuracy = (correct_count / total_count) * 100\n",
        "\n",
        "    # Calculate precision\n",
        "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0) * 100\n",
        "\n",
        "    # Calculate recall\n",
        "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0) * 100\n",
        "\n",
        "    # Inference time of model\n",
        "    fullint_inference_time = np.mean(inference_times)\n",
        "\n",
        "    # Append metrics to lists\n",
        "    int8_model_sizes.append(model_size_mb)\n",
        "    int8_accuracies.append(accuracy)\n",
        "    int8_precisions.append(precision)\n",
        "    int8_recalls.append(recall)\n",
        "    int8_inference_times.append(fullint_inference_time)\n",
        "\n",
        "mean_model_size = np.mean(int8_model_sizes)\n",
        "std_model_size = np.std(int8_model_sizes)\n",
        "\n",
        "mean_accuracy = np.mean(int8_accuracies)\n",
        "std_accuracy = np.std(int8_accuracies)\n",
        "\n",
        "mean_precision = np.mean(int8_precisions)\n",
        "std_precision = np.std(int8_precisions)\n",
        "\n",
        "mean_recall = np.mean(int8_recalls)\n",
        "std_recall = np.std(int8_recalls)\n",
        "\n",
        "mean_inference_time = np.mean(int8_inference_times)\n",
        "std_inference_time = np.std(int8_inference_times)\n",
        "\n",
        "# Print the mean and standard deviation\n",
        "print(f\"Average Int8 Model Size: {mean_model_size} Mb, Std Dev Int8 Model Size: {std_model_size} Mb\")\n",
        "print(f\"Average Int8 Accuracy: {mean_accuracy}, Std Dev Int8 Accuracy: {std_accuracy}\")\n",
        "print(f\"Average Int8 Precision: {mean_precision}, Std Dev Int8 Precision: {std_precision}\")\n",
        "print(f\"Average Int8 Recall: {mean_recall}, Std Dev Int8 Recall: {std_recall}\")\n",
        "print(f\"Average Int8 Inference Time: {mean_inference_time} seconds, Std Dev Int8 Inference Time: {std_inference_time} seconds\")\n",
        "print(\" \")\n",
        "\n",
        "print(\"Int8 Model sizes: \", int8_model_sizes)\n",
        "print(\"Int8 Accuracies: \", int8_accuracies)\n",
        "print(\"Int8 Precisions: \", int8_precisions)\n",
        "print(\"Int8 Recalls: \", int8_recalls)\n",
        "print(\"Int8 Inference Times: \", int8_inference_times)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
